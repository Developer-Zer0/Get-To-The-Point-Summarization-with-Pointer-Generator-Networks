{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pgen_Pointer_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYoIaaQHTCaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from gensim.models import FastText\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFR8vdfvCNp",
        "colab_type": "code",
        "outputId": "2d98d8a8-6baa-474c-a921-fa54dc63d907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9X_CzOXTXzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5cf42e2-f540-44e1-cdf5-7c2fd9d96140"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Amazon_Food/Reviews.csv')\n",
        "len(data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpO8IrRwLB0z",
        "colab_type": "code",
        "outputId": "911d24bd-7ec3-46bd-cd54-d6e0d270dda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lqLjyLLDXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data['Text']\n",
        "y = data['Summary']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BlfwPHtNXI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "  text = str(text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\'s',r'\\tis',text)\n",
        "  text = re.sub(r'\\'ll',r'\\twill',text)\n",
        "  text = re.sub(r'\\'m',r'\\tam',text)\n",
        "  text = re.sub(r'\\'re',r'\\tare',text)\n",
        "  text = re.sub(r'\\'d',r'\\twould',text)\n",
        "  text = re.sub(r'n\\'t',r'\\tnot',text)\n",
        "  text = re.sub('[^a-zA-Z0-9]',' ',text) \n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlDsLdtpNzCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9cc2eaaf-d3bc-4a65-d435-92fac1106bcf"
      },
      "source": [
        "cleaned_source = list(map(clean,x))\n",
        "cleaned_summary = list(map(clean,y))\n",
        "\n",
        "for i in range(len(cleaned_summary)):\n",
        "    cleaned_summary[i] = \"<START> \" + cleaned_summary[i] + \" <END>\"\n",
        "    \n",
        "print(cleaned_source[11])\n",
        "print(cleaned_summary[11])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one of my boys needed to lose some weight and the other did not   i put this food on the floor for the chubby guy  and the protein rich  no by product food up higher where only my skinny boy can jump   the higher food sits going stale   they both really go for this food   and my chubby boy has been losing about an ounce a week \n",
            "<START> my cats love this  diet  food better than their regular food <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGl3foGWa3kG",
        "colab_type": "code",
        "outputId": "85b30ab7-ac58-487e-b200-05f29a05d921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "min_source_length = 1999999\n",
        "max_source_length = 0\n",
        "min_target_length = 199999\n",
        "max_target_length = 0\n",
        "\n",
        "for i in range(len(data)):\n",
        "  min_source_length = min(min_source_length,len(cleaned_source[i].split()))\n",
        "  min_target_length = min(min_target_length,len(cleaned_summary[i].split()))\n",
        "  max_source_length = max(max_source_length,len(cleaned_source[i].split()))\n",
        "  max_target_length = max(max_target_length,len(cleaned_summary[i].split()))\n",
        "\n",
        "print(\"Minimum source length is:  \",min_source_length)\n",
        "print(\"Minimum target length is:  \",min_target_length)\n",
        "print(\"Maximum source length is:  \",max_source_length)\n",
        "print(\"Maximum target length is:  \",max_target_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum source length is:   3\n",
            "Minimum target length is:   2\n",
            "Maximum source length is:   3529\n",
            "Maximum target length is:   50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5eFwEK1bGnt",
        "colab_type": "code",
        "outputId": "6aff0300-58e6-4d8c-b467-2a124f4117ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "new_source = []\n",
        "new_summary = []\n",
        "\n",
        "for i in range(len(cleaned_source)):\n",
        "  if len(cleaned_source[i].split()) <= 50 and len(cleaned_summary[i].split()) <= 15 :\n",
        "    new_source.append(cleaned_source[i])\n",
        "    new_summary.append(cleaned_summary[i])\n",
        "\n",
        "max_source_length = 50\n",
        "max_summary_length = 15\n",
        "\n",
        "print(len(new_source))\n",
        "print(len(new_summary))\n",
        "\n",
        "new_source = new_source[:30000]\n",
        "new_summary = new_summary[:30000]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "245024\n",
            "245024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG4IWgYoXLam",
        "colab_type": "code",
        "outputId": "64934c86-522b-43b4-c18b-953055379107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentences = new_source + new_summary\n",
        "sent_ted = []\n",
        "for sent in sentences:\n",
        "  sent_ted_child = sent.split()\n",
        "  sent_ted.append(sent_ted_child)\n",
        "\n",
        "print(sent_ted[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'have', 'bought', 'several', 'of', 'the', 'vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', 'the', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', 'my', 'labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir5TQ3vGZxNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from gensim.models import FastText\n",
        "# model_ted = FastText(sent_ted, size=128, window=3, min_count=1, workers=4,sg=1, iter=1500)\n",
        "\n",
        "# import pickle\n",
        "# pickle.dump(model_ted, open('128_emb_1lakhdata.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvT4r7pNdzCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0220b6f6-ab61-41b7-90bd-28035b5b21a6"
      },
      "source": [
        "import pickle\n",
        "model_ted = pickle.load(open('/content/drive/My Drive/128_emb.pkl', 'rb'))\n",
        "weights = model_ted.wv\n",
        "print(model_ted.wv.most_similar(\"milk\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('skim', 0.5355638265609741), ('milks', 0.5320084095001221), ('water', 0.5292779207229614), ('foggy', 0.5185288190841675), ('creamer', 0.5173667669296265), ('wipping', 0.5089503526687622), ('raisan', 0.5022526383399963), ('floz', 0.4988960325717926), ('truvia', 0.4962727725505829), ('frother', 0.4953957200050354)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otf9wqD83OxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict \n",
        "\n",
        "word2Index_enc = {}\n",
        "word2Index_dec = {}\n",
        "word2Index_dec_big = {}\n",
        "\n",
        "ind2Word_enc = {}\n",
        "ind2Word_dec = {}\n",
        "ind2Word_dec_big = {}\n",
        "\n",
        "word2PsuInd_dec = {}\n",
        "psuInd2Word_dec = {}\n",
        "\n",
        "encoder_paragraph = list(set((' '.join(new_source)).split()))\n",
        "\n",
        "decoder_paragraph_list = list((' '.join(new_summary)).split())\n",
        "decoder_dict = OrderedDict()\n",
        "for word in decoder_paragraph_list:\n",
        "  try:\n",
        "    decoder_dict[word] = decoder_dict[word] + 1\n",
        "  except:\n",
        "    decoder_dict[word] = 1\n",
        "\n",
        "ind2Word_enc[0] = '<UNK>'\n",
        "ind2Word_dec[0] = '<UNK>'\n",
        "word2Index_enc['<UNK>'] = 0\n",
        "word2Index_dec['<UNK>'] = 0\n",
        "ind2Word_dec_big[0] = '<UNK>'\n",
        "word2Index_dec_big['<UNK>'] = 0\n",
        "word2PsuInd_dec['<UNK>'] = 0\n",
        "psuInd2Word_dec[0] = '<UNK>'\n",
        "\n",
        "dec_index = 1\n",
        "for (decoder_dict_word, decoder_dict_number) in decoder_dict.items():\n",
        "  word2Index_dec_big[decoder_dict_word] = dec_index\n",
        "  ind2Word_dec_big[dec_index] = decoder_dict_word\n",
        "  if decoder_dict_number >= 3 :\n",
        "    word2Index_dec[decoder_dict_word] = dec_index\n",
        "    ind2Word_dec[dec_index] = decoder_dict_word\n",
        "    psuedo_index = len(word2PsuInd_dec.keys())\n",
        "    word2PsuInd_dec[decoder_dict_word] = psuedo_index\n",
        "    psuInd2Word_dec[psuedo_index] = decoder_dict_word\n",
        "  dec_index+=1\n",
        "\n",
        "enc_index = 1\n",
        "for index,word in enumerate(encoder_paragraph):\n",
        "  if word != ' ':\n",
        "    word2Index_enc[word] = enc_index\n",
        "    ind2Word_enc[enc_index] = word \n",
        "    enc_index+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekfBKK_pRm5n",
        "colab_type": "code",
        "outputId": "06689211-95d0-4ab1-f7cb-2a6f9901be6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Decoder vocabulary size\",len(word2Index_dec.keys()))\n",
        "print(\"Encoder vocabulary size\",len(word2Index_enc.keys()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder vocabulary size 2786\n",
            "Encoder vocabulary size 18827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9BR3MZbR5D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = [[word2Index_enc[word] for word in sentence.split() if word in word2Index_enc.keys()] for sentence in new_source ]\n",
        "decoder_input = [[word2Index_dec_big[word] for word in sentence.split() if word in word2Index_dec_big.keys()] for sentence in new_summary ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Xrg0kzCW5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_tensor = [torch.tensor(li,dtype=torch.long,device=device).view(-1, 1) for li in encoder_input]\n",
        "decoder_tensor = [torch.tensor(li,dtype=torch.long,device=device).view(-1, 1) for li in decoder_input]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErsV--5vShLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_vocab_size,hidden_size,num_layers=1,bidirectional=False):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.bidirectional = bidirectional\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.gru_layer = nn.GRU(input_size = self.hidden_size,hidden_size = self.hidden_size,num_layers = self.num_layers)\n",
        "\n",
        "  def forward(self,input_,prev_hidden_state):\n",
        "    input_word = ind2Word_enc[input_.data.tolist()[0]]\n",
        "    embedded_outputs = torch.tensor(weights[input_word], device = device).view(1,1,-1)\n",
        "    output,prev_hidden_state = self.gru_layer(embedded_outputs,prev_hidden_state)  #output is batch_size times hidden_size\n",
        "    return output,prev_hidden_state\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size,device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I29Rw8CA7esY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "  def __init__(self,output_vocab_size,hidden_size,max_length_encoder,dropout_value,num_layers=1):\n",
        "      super(AttentionDecoder,self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "      self.output_vocab_size = output_vocab_size\n",
        "      self.dropout_p = dropout_value\n",
        "      self.max_length_encoder = max_length_encoder\n",
        "      self.embedding_layer = nn.Embedding(self.output_vocab_size,self.hidden_size)\n",
        "      self.attention_layer = nn.Linear(self.hidden_size*2,self.max_length_encoder)\n",
        "      self.attention_combine = nn.Linear(self.hidden_size*2,self.hidden_size)\n",
        "\n",
        "      self.s_layer = nn.Linear(self.hidden_size, 1)\n",
        "      self.x_layer = nn.Linear(self.hidden_size, 1)\n",
        "      self.context_layer = nn.Linear(self.hidden_size, 1)\n",
        "      self.linear_pgen = nn.Linear(3, 1)\n",
        "\n",
        "      self.gru_layer = nn.GRU(self.hidden_size,self.hidden_size)\n",
        "      self.output_layer = nn.Linear(self.hidden_size,self.output_vocab_size)\n",
        "      self.dropout_layer = nn.Dropout(self.dropout_p)    \n",
        "\n",
        "  def forward(self,input_,prev_hidden_state,encoder_output,prev_unk_word):\n",
        "      input_word = ind2Word_dec_big[input_.data.tolist()[0]]\n",
        "      if input_word == '<UNK>':\n",
        "        embedded_outputs = torch.tensor(weights[prev_unk_word], device = device).view(1,1,-1)\n",
        "      else:\n",
        "        embedded_outputs = torch.tensor(weights[input_word], device = device).view(1,1,-1)\n",
        "        \n",
        "      embeddings_dropout = self.dropout_layer(embedded_outputs)\n",
        "      attention_layer_output = self.attention_layer(torch.cat((embeddings_dropout[0],prev_hidden_state[0]),1))\n",
        "      attention_weights = nn.functional.softmax(attention_layer_output,dim=1)\n",
        "      attention_applied = torch.bmm(attention_weights.unsqueeze(0),encoder_output.unsqueeze(0))\n",
        "      attention_combine_logits = self.attention_combine(torch.cat((embeddings_dropout[0],attention_applied[0]),1)).unsqueeze(0)  #since gru requires a batch dimension\n",
        "      attention_combine_relu = nn.functional.relu(attention_combine_logits)\n",
        "\n",
        "      s_output = self.s_layer(prev_hidden_state[0])\n",
        "      x_output = self.x_layer(embeddings_dropout[0])\n",
        "      context = torch.flatten(attention_applied)\n",
        "      context_weights = self.context_layer(attention_applied)\n",
        "      sx = torch.cat((s_output[0],x_output[0]),0)\n",
        "      sxc = torch.cat((sx,context_weights[0][0]),0)\n",
        "      linear_pgen = self.linear_pgen(sxc)\n",
        "      m = nn.Sigmoid()\n",
        "      pgen = m(linear_pgen)\n",
        "\n",
        "      output,hidden = self.gru_layer(attention_combine_relu,prev_hidden_state)\n",
        "      output_logits = self.output_layer(output)\n",
        "      output_softmax = nn.functional.log_softmax(output_logits[0],dim=1)\n",
        "      return output_softmax,hidden,attention_weights,pgen\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size,device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y40789vQM8gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(encoder, decoder, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion, max_length, iters):\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  prev_unk_word = ''\n",
        "\n",
        "  encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "\n",
        "  input_length = input_tensor.size(0)\n",
        "  output_length = target_tensor.size(0)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for encoder_index in range(0, input_length):\n",
        "    encoder_output,encoder_hidden = encoder(input_tensor[encoder_index], encoder_hidden)\n",
        "    encoder_outputs[encoder_index] = encoder_output[0,0]\n",
        "\n",
        "  decoder_input = torch.tensor([word2Index_dec['<START>']],device=device)   \n",
        "  decoder_hidden = encoder_hidden\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "  extended_vocab = psuInd2Word_dec.copy()\n",
        "  reverse_extended_vocab = word2PsuInd_dec.copy()\n",
        "  duplicate_words = {}\n",
        "  extend_key = len(word2Index_dec.keys())\n",
        "  input_list = input_tensor.tolist()\n",
        "  i =0\n",
        "  for input_word in input_list:\n",
        "    if ind2Word_enc[input_word[0]] in word2Index_dec.keys():\n",
        "      duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word[0]]]\n",
        "    else:\n",
        "      extended_vocab[extend_key] = ind2Word_enc[input_word[0]]\n",
        "      reverse_extended_vocab[ind2Word_enc[input_word[0]]] = extend_key\n",
        "      extend_key += 1\n",
        "    i = i+1\n",
        "\n",
        "  if use_teacher_forcing:\n",
        "    for decoder_index in range(output_length):\n",
        "      decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs,prev_unk_word)\n",
        "      P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
        "\n",
        "      decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
        "      p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
        "      p_duplicate_list = p_duplicate_list.tolist()\n",
        "      for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
        "        p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
        "      p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
        "      p_diag = torch.mm(decoder_attention, p_duplicate)\n",
        "      p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
        "      p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
        "      P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
        "\n",
        "      for i in range(input_length):\n",
        "        if not (1 in p_duplicate_list[i]):\n",
        "          P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
        "\n",
        "      try:\n",
        "        loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
        "        loss.backward(retain_graph=True)\n",
        "      except KeyError:\n",
        "        loss += torch.tensor(0,dtype=torch.float,device=device)\n",
        "      decoder_input = target_tensor[decoder_index]\n",
        "  else:\n",
        "\n",
        "    for decoder_index in range(output_length):\n",
        "      decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs,prev_unk_word) \n",
        "      P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
        "\n",
        "      decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
        "      p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
        "      p_duplicate_list = p_duplicate_list.tolist()\n",
        "      for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
        "        p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
        "      p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
        "      p_diag = torch.mm(decoder_attention, p_duplicate)\n",
        "      p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
        "      p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
        "      P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
        "\n",
        "      for i in range(input_length):\n",
        "        if not (1 in p_duplicate_list[i]):\n",
        "          P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
        "\n",
        "      try:\n",
        "        loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
        "        loss.backward(retain_graph=True)\n",
        "      except KeyError:\n",
        "        loss += torch.tensor(0,dtype=torch.float,device=device)\n",
        "      idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
        "      if idx.item() < len(word2Index_dec.keys()):   \n",
        "        decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
        "      elif idx.item() >= len(word2Index_dec.keys()):\n",
        "        prev_unk_word = extended_vocab[idx.item()]\n",
        "        decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
        "      elif (decoder_input.item() == word2Index_dec['<END>']):\n",
        "        break\n",
        "\n",
        "  if iters > 20000:\n",
        "    torch.nn.utils.clip_grad_norm_(rnn_encoder.parameters(),0.4)\n",
        "    torch.nn.utils.clip_grad_norm_(rnn_decoder.parameters(),0.4)\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item()/output_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocM_vNF6KTWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    if percent != 0:\n",
        "      es = s / (percent)\n",
        "      rs = es - s\n",
        "      return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQnTrJ0SLSYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = np.arange(len(encoder_tensor))\n",
        "np.random.shuffle(arr)\n",
        "len(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBToTQxGLXgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary for creating loss graph\n",
        "loss_graph = {}\n",
        "\n",
        "def train_Iters(encoder,decoder,n_iters,print_every=50, plot_every=100,learning_rate = 0.03):\n",
        "  # start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0  # Reset every print_every\n",
        "  plot_loss_total = 0\n",
        "\n",
        "  encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "  training_pairs = [random.choice(pairs) for i in range(n_iters)]\n",
        "  \n",
        "  criterion = nn.NLLLoss()\n",
        "  for iters in range(n_iters):\n",
        "    training_pair = training_pairs[iters - 1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "\n",
        "    input_tensor = torch.tensor(input_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
        "    target_tensor = torch.tensor(target_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
        "\n",
        "    loss = train(encoder,decoder,input_tensor,target_tensor,encoder_optimizer,decoder_optimizer,criterion,max_source_length, iters)\n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "\n",
        "    if iters % print_every == 0:\n",
        "        print_loss_avg = print_loss_total / print_every\n",
        "        print_loss_total = 0\n",
        "        print('%s %d%%) %.4f' % (iters, iters / len(arr) * 100, print_loss_avg))\n",
        "\n",
        "        if iters > 0:\n",
        "          loss_graph[iters] = print_loss_avg\n",
        "\n",
        "    if iters % plot_every == 0:\n",
        "        plot_loss_avg = plot_loss_total / plot_every\n",
        "        plot_losses.append(plot_loss_avg)\n",
        "        plot_loss_total = 0\n",
        "\n",
        "  # showPlot(plot_losses)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDEs3lSzLcrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgo_fbnFLgxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = []\n",
        "for enc,dec in zip(encoder_input,decoder_input):\n",
        "    pairs.append([enc,dec])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qPlKIYDLi_l",
        "colab_type": "code",
        "outputId": "3a0f8dff-c6b2-4fc2-95c7-c601bf7b8b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_size = 128\n",
        "rnn_encoder = Encoder(len(word2Index_enc.keys()),hidden_size).to(device=device)\n",
        "rnn_decoder = AttentionDecoder(len(word2Index_dec.keys()),hidden_size,max_source_length,0.2).to(device=device)\n",
        "\n",
        "train_Iters(rnn_encoder,rnn_decoder,100000)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0%) 0.1768\n",
            "50 0%) 5.4431\n",
            "100 0%) 4.8154\n",
            "150 0%) 4.5362\n",
            "200 0%) 4.7026\n",
            "250 0%) 4.7144\n",
            "300 1%) 4.5535\n",
            "350 1%) 4.2123\n",
            "400 1%) 4.2582\n",
            "450 1%) 4.6562\n",
            "500 1%) 4.3248\n",
            "550 1%) 4.3570\n",
            "600 2%) 4.1034\n",
            "650 2%) 4.4397\n",
            "700 2%) 4.1432\n",
            "750 2%) 4.2782\n",
            "800 2%) 4.2599\n",
            "850 2%) 4.2046\n",
            "900 3%) 4.0812\n",
            "950 3%) 4.0749\n",
            "1000 3%) 4.1208\n",
            "1050 3%) 4.2579\n",
            "1100 3%) 4.1653\n",
            "1150 3%) 3.9679\n",
            "1200 4%) 4.3853\n",
            "1250 4%) 4.3869\n",
            "1300 4%) 3.9935\n",
            "1350 4%) 3.8368\n",
            "1400 4%) 3.9361\n",
            "1450 4%) 4.0907\n",
            "1500 5%) 3.9974\n",
            "1550 5%) 3.9242\n",
            "1600 5%) 4.0037\n",
            "1650 5%) 4.1252\n",
            "1700 5%) 4.1093\n",
            "1750 5%) 3.9566\n",
            "1800 6%) 3.9977\n",
            "1850 6%) 4.2370\n",
            "1900 6%) 4.0446\n",
            "1950 6%) 4.0646\n",
            "2000 6%) 4.1106\n",
            "2050 6%) 3.9392\n",
            "2100 7%) 4.2062\n",
            "2150 7%) 4.3813\n",
            "2200 7%) 4.4225\n",
            "2250 7%) 4.0551\n",
            "2300 7%) 3.8839\n",
            "2350 7%) 3.9797\n",
            "2400 8%) 3.9289\n",
            "2450 8%) 3.8677\n",
            "2500 8%) 3.7976\n",
            "2550 8%) 3.8575\n",
            "2600 8%) 3.8552\n",
            "2650 8%) 3.9993\n",
            "2700 9%) 3.9557\n",
            "2750 9%) 3.8115\n",
            "2800 9%) 4.1785\n",
            "2850 9%) 4.0981\n",
            "2900 9%) 3.7647\n",
            "2950 9%) 3.8231\n",
            "3000 10%) 4.0047\n",
            "3050 10%) 4.3761\n",
            "3100 10%) 4.1745\n",
            "3150 10%) 3.9164\n",
            "3200 10%) 4.1695\n",
            "3250 10%) 3.7670\n",
            "3300 11%) 4.2612\n",
            "3350 11%) 3.9580\n",
            "3400 11%) 4.0140\n",
            "3450 11%) 4.0378\n",
            "3500 11%) 3.7597\n",
            "3550 11%) 3.9435\n",
            "3600 12%) 3.6356\n",
            "3650 12%) 3.7926\n",
            "3700 12%) 3.8408\n",
            "3750 12%) 3.8057\n",
            "3800 12%) 3.7536\n",
            "3850 12%) 4.3878\n",
            "3900 13%) 3.8259\n",
            "3950 13%) 3.8996\n",
            "4000 13%) 3.7058\n",
            "4050 13%) 3.7832\n",
            "4100 13%) 4.3018\n",
            "4150 13%) 4.1427\n",
            "4200 14%) 3.9985\n",
            "4250 14%) 4.1782\n",
            "4300 14%) 3.4301\n",
            "4350 14%) 3.8438\n",
            "4400 14%) 3.8799\n",
            "4450 14%) 3.9131\n",
            "4500 15%) 4.3249\n",
            "4550 15%) 4.0853\n",
            "4600 15%) 3.9229\n",
            "4650 15%) 3.7217\n",
            "4700 15%) 3.7988\n",
            "4750 15%) 4.2025\n",
            "4800 16%) 3.9275\n",
            "4850 16%) 4.1149\n",
            "4900 16%) 4.0739\n",
            "4950 16%) 3.7764\n",
            "5000 16%) 4.2251\n",
            "5050 16%) 4.0437\n",
            "5100 17%) 4.1660\n",
            "5150 17%) 3.9968\n",
            "5200 17%) 3.5086\n",
            "5250 17%) 3.9319\n",
            "5300 17%) 3.7422\n",
            "5350 17%) 3.9581\n",
            "5400 18%) 3.7102\n",
            "5450 18%) 3.9105\n",
            "5500 18%) 3.7611\n",
            "5550 18%) 4.3920\n",
            "5600 18%) 3.8794\n",
            "5650 18%) 3.7418\n",
            "5700 19%) 3.5473\n",
            "5750 19%) 3.8282\n",
            "5800 19%) 3.9333\n",
            "5850 19%) 3.5127\n",
            "5900 19%) 3.9362\n",
            "5950 19%) 4.1235\n",
            "6000 20%) 3.8953\n",
            "6050 20%) 3.7772\n",
            "6100 20%) 4.0635\n",
            "6150 20%) 3.9791\n",
            "6200 20%) 3.6809\n",
            "6250 20%) 3.8487\n",
            "6300 21%) 3.6648\n",
            "6350 21%) 3.6982\n",
            "6400 21%) 3.7023\n",
            "6450 21%) 3.8371\n",
            "6500 21%) 4.1850\n",
            "6550 21%) 3.6321\n",
            "6600 22%) 4.0332\n",
            "6650 22%) 4.0903\n",
            "6700 22%) 4.0585\n",
            "6750 22%) 3.6130\n",
            "6800 22%) 3.9365\n",
            "6850 22%) 3.6911\n",
            "6900 23%) 3.8567\n",
            "6950 23%) 3.6456\n",
            "7000 23%) 3.8242\n",
            "7050 23%) 3.4450\n",
            "7100 23%) 4.1446\n",
            "7150 23%) 4.1223\n",
            "7200 24%) 4.0214\n",
            "7250 24%) 3.7249\n",
            "7300 24%) 3.8437\n",
            "7350 24%) 4.0446\n",
            "7400 24%) 3.7568\n",
            "7450 24%) 3.7174\n",
            "7500 25%) 3.9589\n",
            "7550 25%) 3.6311\n",
            "7600 25%) 3.8055\n",
            "7650 25%) 3.8531\n",
            "7700 25%) 3.7142\n",
            "7750 25%) 3.8661\n",
            "7800 26%) 4.0092\n",
            "7850 26%) 4.0563\n",
            "7900 26%) 3.9688\n",
            "7950 26%) 3.6889\n",
            "8000 26%) 4.2553\n",
            "8050 26%) 3.7374\n",
            "8100 27%) 4.2664\n",
            "8150 27%) 3.8023\n",
            "8200 27%) 3.9789\n",
            "8250 27%) 3.6661\n",
            "8300 27%) 3.9957\n",
            "8350 27%) 4.1159\n",
            "8400 28%) 3.6990\n",
            "8450 28%) 3.5388\n",
            "8500 28%) 3.8735\n",
            "8550 28%) 3.9149\n",
            "8600 28%) 4.0969\n",
            "8650 28%) 3.7997\n",
            "8700 28%) 3.9617\n",
            "8750 29%) 3.9054\n",
            "8800 29%) 3.8882\n",
            "8850 29%) 3.9795\n",
            "8900 29%) 4.0550\n",
            "8950 29%) 3.7997\n",
            "9000 30%) 3.6202\n",
            "9050 30%) 4.2755\n",
            "9100 30%) 3.9402\n",
            "9150 30%) 3.9808\n",
            "9200 30%) 3.9678\n",
            "9250 30%) 3.8654\n",
            "9300 31%) 3.8714\n",
            "9350 31%) 3.9302\n",
            "9400 31%) 3.8520\n",
            "9450 31%) 3.9893\n",
            "9500 31%) 3.3736\n",
            "9550 31%) 3.8896\n",
            "9600 32%) 3.8097\n",
            "9650 32%) 3.9713\n",
            "9700 32%) 3.6782\n",
            "9750 32%) 3.8654\n",
            "9800 32%) 3.8637\n",
            "9850 32%) 3.8631\n",
            "9900 33%) 3.5881\n",
            "9950 33%) 4.0263\n",
            "10000 33%) 4.0199\n",
            "10050 33%) 3.6023\n",
            "10100 33%) 3.9030\n",
            "10150 33%) 3.9190\n",
            "10200 34%) 3.6722\n",
            "10250 34%) 3.5837\n",
            "10300 34%) 3.9015\n",
            "10350 34%) 3.9361\n",
            "10400 34%) 3.8722\n",
            "10450 34%) 3.8687\n",
            "10500 35%) 3.5564\n",
            "10550 35%) 3.6833\n",
            "10600 35%) 3.7897\n",
            "10650 35%) 4.0292\n",
            "10700 35%) 3.5986\n",
            "10750 35%) 4.0575\n",
            "10800 36%) 3.5718\n",
            "10850 36%) 3.8987\n",
            "10900 36%) 3.6697\n",
            "10950 36%) 3.7164\n",
            "11000 36%) 4.1151\n",
            "11050 36%) 3.6148\n",
            "11100 37%) 3.7917\n",
            "11150 37%) 3.8249\n",
            "11200 37%) 3.9153\n",
            "11250 37%) 3.4780\n",
            "11300 37%) 3.8543\n",
            "11350 37%) 4.0974\n",
            "11400 38%) 4.2220\n",
            "11450 38%) 3.7797\n",
            "11500 38%) 3.7674\n",
            "11550 38%) 3.8219\n",
            "11600 38%) 3.6776\n",
            "11650 38%) 3.8852\n",
            "11700 39%) 3.6286\n",
            "11750 39%) 4.0046\n",
            "11800 39%) 3.7205\n",
            "11850 39%) 3.5547\n",
            "11900 39%) 3.5902\n",
            "11950 39%) 3.8465\n",
            "12000 40%) 3.8124\n",
            "12050 40%) 3.9273\n",
            "12100 40%) 3.5232\n",
            "12150 40%) 3.7316\n",
            "12200 40%) 3.8173\n",
            "12250 40%) 3.8641\n",
            "12300 41%) 3.8607\n",
            "12350 41%) 3.7017\n",
            "12400 41%) 3.8571\n",
            "12450 41%) 3.8566\n",
            "12500 41%) 3.9163\n",
            "12550 41%) 3.8152\n",
            "12600 42%) 3.4444\n",
            "12650 42%) 3.6981\n",
            "12700 42%) 3.6661\n",
            "12750 42%) 4.0034\n",
            "12800 42%) 3.4456\n",
            "12850 42%) 3.5739\n",
            "12900 43%) 3.0721\n",
            "12950 43%) 3.4285\n",
            "13000 43%) 3.8086\n",
            "13050 43%) 4.1656\n",
            "13100 43%) 3.5260\n",
            "13150 43%) 3.9491\n",
            "13200 44%) 3.8921\n",
            "13250 44%) 3.8445\n",
            "13300 44%) 3.7750\n",
            "13350 44%) 3.8463\n",
            "13400 44%) 3.9256\n",
            "13450 44%) 3.5690\n",
            "13500 45%) 4.1822\n",
            "13550 45%) 3.4949\n",
            "13600 45%) 4.0464\n",
            "13650 45%) 4.0667\n",
            "13700 45%) 3.7768\n",
            "13750 45%) 3.5388\n",
            "13800 46%) 3.8476\n",
            "13850 46%) 3.9456\n",
            "13900 46%) 3.8564\n",
            "13950 46%) 3.7942\n",
            "14000 46%) 3.5275\n",
            "14050 46%) 3.8888\n",
            "14100 47%) 3.9759\n",
            "14150 47%) 3.7587\n",
            "14200 47%) 3.7437\n",
            "14250 47%) 3.4391\n",
            "14300 47%) 3.5197\n",
            "14350 47%) 3.5910\n",
            "14400 48%) 3.7701\n",
            "14450 48%) 3.6955\n",
            "14500 48%) 3.8850\n",
            "14550 48%) 3.6479\n",
            "14600 48%) 3.9000\n",
            "14650 48%) 3.5622\n",
            "14700 49%) 3.7571\n",
            "14750 49%) 3.5979\n",
            "14800 49%) 3.6627\n",
            "14850 49%) 3.9885\n",
            "14900 49%) 3.2324\n",
            "14950 49%) 3.5451\n",
            "15000 50%) 3.6976\n",
            "15050 50%) 3.8744\n",
            "15100 50%) 3.8066\n",
            "15150 50%) 3.5170\n",
            "15200 50%) 3.4577\n",
            "15250 50%) 4.0901\n",
            "15300 51%) 3.7103\n",
            "15350 51%) 3.4939\n",
            "15400 51%) 3.7451\n",
            "15450 51%) 3.5119\n",
            "15500 51%) 3.5526\n",
            "15550 51%) 3.3470\n",
            "15600 52%) 4.0018\n",
            "15650 52%) 3.6934\n",
            "15700 52%) 3.3264\n",
            "15750 52%) 3.8641\n",
            "15800 52%) 3.8613\n",
            "15850 52%) 3.8331\n",
            "15900 53%) 3.6983\n",
            "15950 53%) 3.5552\n",
            "16000 53%) 3.5753\n",
            "16050 53%) 3.7532\n",
            "16100 53%) 3.5572\n",
            "16150 53%) 3.4473\n",
            "16200 54%) 3.6438\n",
            "16250 54%) 3.8613\n",
            "16300 54%) 3.7275\n",
            "16350 54%) 3.5143\n",
            "16400 54%) 3.8572\n",
            "16450 54%) 3.5599\n",
            "16500 55%) 3.6582\n",
            "16550 55%) 3.2978\n",
            "16600 55%) 3.7014\n",
            "16650 55%) 3.7724\n",
            "16700 55%) 3.6898\n",
            "16750 55%) 3.5457\n",
            "16800 56%) 3.9625\n",
            "16850 56%) 3.9002\n",
            "16900 56%) 3.6446\n",
            "16950 56%) 3.8382\n",
            "17000 56%) 3.7291\n",
            "17050 56%) 3.5445\n",
            "17100 56%) 3.4271\n",
            "17150 57%) 3.8520\n",
            "17200 57%) 3.5318\n",
            "17250 57%) 3.9394\n",
            "17300 57%) 3.5660\n",
            "17350 57%) 3.8613\n",
            "17400 57%) 3.6639\n",
            "17450 58%) 3.4964\n",
            "17500 58%) 3.4835\n",
            "17550 58%) 3.8842\n",
            "17600 58%) 3.9754\n",
            "17650 58%) 3.5616\n",
            "17700 59%) 3.4885\n",
            "17750 59%) 3.4564\n",
            "17800 59%) 3.7674\n",
            "17850 59%) 3.6415\n",
            "17900 59%) 3.8183\n",
            "17950 59%) 3.7678\n",
            "18000 60%) 3.8900\n",
            "18050 60%) 3.8336\n",
            "18100 60%) 3.4711\n",
            "18150 60%) 3.4545\n",
            "18200 60%) 3.5045\n",
            "18250 60%) 3.4487\n",
            "18300 61%) 3.4140\n",
            "18350 61%) 3.7309\n",
            "18400 61%) 3.7692\n",
            "18450 61%) 3.6604\n",
            "18500 61%) 3.4615\n",
            "18550 61%) 3.5239\n",
            "18600 62%) 3.6020\n",
            "18650 62%) 3.7113\n",
            "18700 62%) 3.9116\n",
            "18750 62%) 3.5551\n",
            "18800 62%) 3.4847\n",
            "18850 62%) 3.8967\n",
            "18900 63%) 3.5954\n",
            "18950 63%) 3.8156\n",
            "19000 63%) 3.8806\n",
            "19050 63%) 3.7528\n",
            "19100 63%) 3.7328\n",
            "19150 63%) 3.6806\n",
            "19200 64%) 3.7152\n",
            "19250 64%) 3.8302\n",
            "19300 64%) 3.5176\n",
            "19350 64%) 3.9077\n",
            "19400 64%) 3.9298\n",
            "19450 64%) 3.8218\n",
            "19500 65%) 3.4419\n",
            "19550 65%) 3.8076\n",
            "19600 65%) 3.6759\n",
            "19650 65%) 3.7176\n",
            "19700 65%) 3.7888\n",
            "19750 65%) 3.6282\n",
            "19800 66%) 3.6633\n",
            "19850 66%) 3.8953\n",
            "19900 66%) 3.8056\n",
            "19950 66%) 3.9550\n",
            "20000 66%) 3.5677\n",
            "20050 66%) 3.8260\n",
            "20100 67%) 3.6427\n",
            "20150 67%) 3.7564\n",
            "20200 67%) 3.9368\n",
            "20250 67%) 3.7621\n",
            "20300 67%) 3.8576\n",
            "20350 67%) 3.6418\n",
            "20400 68%) 3.5278\n",
            "20450 68%) 3.3810\n",
            "20500 68%) 3.5413\n",
            "20550 68%) 3.5118\n",
            "20600 68%) 3.5064\n",
            "20650 68%) 3.5562\n",
            "20700 69%) 3.5446\n",
            "20750 69%) 3.6420\n",
            "20800 69%) 3.6253\n",
            "20850 69%) 3.5377\n",
            "20900 69%) 3.1249\n",
            "20950 69%) 3.6923\n",
            "21000 70%) 3.4750\n",
            "21050 70%) 3.4217\n",
            "21100 70%) 3.3422\n",
            "21150 70%) 3.5899\n",
            "21200 70%) 3.5661\n",
            "21250 70%) 3.5910\n",
            "21300 71%) 3.6990\n",
            "21350 71%) 3.4093\n",
            "21400 71%) 3.6406\n",
            "21450 71%) 3.4401\n",
            "21500 71%) 3.4904\n",
            "21550 71%) 3.7210\n",
            "21600 72%) 3.5489\n",
            "21650 72%) 3.8617\n",
            "21700 72%) 3.5124\n",
            "21750 72%) 3.2181\n",
            "21800 72%) 3.5504\n",
            "21850 72%) 3.1784\n",
            "21900 73%) 3.5971\n",
            "21950 73%) 3.2488\n",
            "22000 73%) 3.2625\n",
            "22050 73%) 3.4223\n",
            "22100 73%) 3.6935\n",
            "22150 73%) 3.6488\n",
            "22200 74%) 3.3838\n",
            "22250 74%) 3.6147\n",
            "22300 74%) 3.4134\n",
            "22350 74%) 3.3313\n",
            "22400 74%) 3.6236\n",
            "22450 74%) 3.0604\n",
            "22500 75%) 3.3480\n",
            "22550 75%) 3.2380\n",
            "22600 75%) 3.5867\n",
            "22650 75%) 3.4109\n",
            "22700 75%) 3.2168\n",
            "22750 75%) 3.1783\n",
            "22800 76%) 3.4413\n",
            "22850 76%) 3.2568\n",
            "22900 76%) 3.4059\n",
            "22950 76%) 3.4815\n",
            "23000 76%) 3.8867\n",
            "23050 76%) 3.8299\n",
            "23100 77%) 3.4192\n",
            "23150 77%) 3.6004\n",
            "23200 77%) 3.3470\n",
            "23250 77%) 3.4264\n",
            "23300 77%) 3.3432\n",
            "23350 77%) 3.1849\n",
            "23400 78%) 3.4884\n",
            "23450 78%) 3.3418\n",
            "23500 78%) 3.6887\n",
            "23550 78%) 3.4673\n",
            "23600 78%) 3.3463\n",
            "23650 78%) 3.6459\n",
            "23700 79%) 3.5391\n",
            "23750 79%) 3.1411\n",
            "23800 79%) 3.6646\n",
            "23850 79%) 3.5969\n",
            "23900 79%) 3.3038\n",
            "23950 79%) 3.6278\n",
            "24000 80%) 3.6746\n",
            "24050 80%) 3.2576\n",
            "24100 80%) 3.3405\n",
            "24150 80%) 3.4662\n",
            "24200 80%) 3.4333\n",
            "24250 80%) 3.6614\n",
            "24300 81%) 3.4107\n",
            "24350 81%) 3.4549\n",
            "24400 81%) 3.4640\n",
            "24450 81%) 3.5362\n",
            "24500 81%) 3.4349\n",
            "24550 81%) 3.2674\n",
            "24600 82%) 3.3731\n",
            "24650 82%) 3.3471\n",
            "24700 82%) 3.4361\n",
            "24750 82%) 3.2127\n",
            "24800 82%) 3.3134\n",
            "24850 82%) 3.6336\n",
            "24900 83%) 3.3391\n",
            "24950 83%) 3.4709\n",
            "25000 83%) 3.8708\n",
            "25050 83%) 3.4804\n",
            "25100 83%) 3.4067\n",
            "25150 83%) 3.8603\n",
            "25200 84%) 3.2030\n",
            "25250 84%) 3.4630\n",
            "25300 84%) 3.2948\n",
            "25350 84%) 3.3388\n",
            "25400 84%) 3.3209\n",
            "25450 84%) 3.6193\n",
            "25500 85%) 3.5154\n",
            "25550 85%) 3.6237\n",
            "25600 85%) 3.0713\n",
            "25650 85%) 3.2439\n",
            "25700 85%) 3.6917\n",
            "25750 85%) 3.2503\n",
            "25800 86%) 3.2705\n",
            "25850 86%) 3.0949\n",
            "25900 86%) 3.4977\n",
            "25950 86%) 3.2572\n",
            "26000 86%) 3.7959\n",
            "26050 86%) 3.1944\n",
            "26100 87%) 3.4626\n",
            "26150 87%) 3.4881\n",
            "26200 87%) 3.5926\n",
            "26250 87%) 3.2671\n",
            "26300 87%) 3.4432\n",
            "26350 87%) 3.5760\n",
            "26400 88%) 3.4981\n",
            "26450 88%) 3.4972\n",
            "26500 88%) 3.3944\n",
            "26550 88%) 3.5384\n",
            "26600 88%) 3.4287\n",
            "26650 88%) 3.1906\n",
            "26700 89%) 3.4325\n",
            "26750 89%) 3.2475\n",
            "26800 89%) 3.1594\n",
            "26850 89%) 3.2471\n",
            "26900 89%) 3.1535\n",
            "26950 89%) 3.4298\n",
            "27000 90%) 3.0451\n",
            "27050 90%) 3.3641\n",
            "27100 90%) 3.5336\n",
            "27150 90%) 3.7428\n",
            "27200 90%) 3.3134\n",
            "27250 90%) 3.5043\n",
            "27300 91%) 3.1178\n",
            "27350 91%) 3.3216\n",
            "27400 91%) 3.4650\n",
            "27450 91%) 3.3057\n",
            "27500 91%) 3.2567\n",
            "27550 91%) 3.4864\n",
            "27600 92%) 3.0650\n",
            "27650 92%) 3.4641\n",
            "27700 92%) 3.5203\n",
            "27750 92%) 3.0906\n",
            "27800 92%) 3.3870\n",
            "27850 92%) 3.5102\n",
            "27900 93%) 3.4954\n",
            "27950 93%) 3.5289\n",
            "28000 93%) 3.3919\n",
            "28050 93%) 3.2450\n",
            "28100 93%) 3.2477\n",
            "28150 93%) 3.4595\n",
            "28200 94%) 3.4052\n",
            "28250 94%) 3.6594\n",
            "28300 94%) 3.2703\n",
            "28350 94%) 3.6500\n",
            "28400 94%) 3.3303\n",
            "28450 94%) 3.8236\n",
            "28500 95%) 3.2099\n",
            "28550 95%) 3.4079\n",
            "28600 95%) 3.4590\n",
            "28650 95%) 3.6640\n",
            "28700 95%) 3.2500\n",
            "28750 95%) 3.5203\n",
            "28800 96%) 3.5459\n",
            "28850 96%) 3.4922\n",
            "28900 96%) 3.3132\n",
            "28950 96%) 3.6365\n",
            "29000 96%) 3.0226\n",
            "29050 96%) 3.4464\n",
            "29100 97%) 3.8799\n",
            "29150 97%) 3.3249\n",
            "29200 97%) 3.5318\n",
            "29250 97%) 3.1802\n",
            "29300 97%) 3.2670\n",
            "29350 97%) 3.3001\n",
            "29400 98%) 3.3790\n",
            "29450 98%) 3.4451\n",
            "29500 98%) 3.2163\n",
            "29550 98%) 3.1363\n",
            "29600 98%) 3.2418\n",
            "29650 98%) 3.3588\n",
            "29700 99%) 3.5846\n",
            "29750 99%) 3.3748\n",
            "29800 99%) 3.6917\n",
            "29850 99%) 3.1025\n",
            "29900 99%) 3.1309\n",
            "29950 99%) 3.3795\n",
            "30000 100%) 3.5107\n",
            "30050 100%) 3.6372\n",
            "30100 100%) 3.4557\n",
            "30150 100%) 3.3700\n",
            "30200 100%) 3.3799\n",
            "30250 100%) 3.4480\n",
            "30300 101%) 3.6661\n",
            "30350 101%) 3.6140\n",
            "30400 101%) 3.2133\n",
            "30450 101%) 3.4696\n",
            "30500 101%) 3.6107\n",
            "30550 101%) 3.6386\n",
            "30600 102%) 3.1633\n",
            "30650 102%) 3.2896\n",
            "30700 102%) 3.4116\n",
            "30750 102%) 3.6610\n",
            "30800 102%) 3.3349\n",
            "30850 102%) 3.3576\n",
            "30900 103%) 3.3475\n",
            "30950 103%) 3.5332\n",
            "31000 103%) 3.6283\n",
            "31050 103%) 3.4905\n",
            "31100 103%) 3.8630\n",
            "31150 103%) 3.1772\n",
            "31200 104%) 3.5543\n",
            "31250 104%) 3.6246\n",
            "31300 104%) 3.3510\n",
            "31350 104%) 3.3039\n",
            "31400 104%) 3.3207\n",
            "31450 104%) 3.5681\n",
            "31500 105%) 3.4256\n",
            "31550 105%) 3.5243\n",
            "31600 105%) 3.1912\n",
            "31650 105%) 3.3872\n",
            "31700 105%) 3.0889\n",
            "31750 105%) 3.5234\n",
            "31800 106%) 3.4833\n",
            "31850 106%) 3.5144\n",
            "31900 106%) 3.3962\n",
            "31950 106%) 3.3324\n",
            "32000 106%) 3.6642\n",
            "32050 106%) 3.2547\n",
            "32100 107%) 3.2801\n",
            "32150 107%) 3.6677\n",
            "32200 107%) 3.4064\n",
            "32250 107%) 3.5750\n",
            "32300 107%) 3.3662\n",
            "32350 107%) 3.1107\n",
            "32400 108%) 3.4118\n",
            "32450 108%) 3.2856\n",
            "32500 108%) 3.3509\n",
            "32550 108%) 3.5320\n",
            "32600 108%) 3.3493\n",
            "32650 108%) 3.3799\n",
            "32700 109%) 3.4416\n",
            "32750 109%) 3.2954\n",
            "32800 109%) 3.3521\n",
            "32850 109%) 3.5503\n",
            "32900 109%) 3.6838\n",
            "32950 109%) 3.2083\n",
            "33000 110%) 3.2047\n",
            "33050 110%) 3.3822\n",
            "33100 110%) 3.4655\n",
            "33150 110%) 3.1587\n",
            "33200 110%) 3.5930\n",
            "33250 110%) 3.4324\n",
            "33300 111%) 3.3490\n",
            "33350 111%) 3.4700\n",
            "33400 111%) 3.4761\n",
            "33450 111%) 3.4166\n",
            "33500 111%) 3.6204\n",
            "33550 111%) 3.3857\n",
            "33600 112%) 3.4301\n",
            "33650 112%) 3.3379\n",
            "33700 112%) 3.2829\n",
            "33750 112%) 3.6287\n",
            "33800 112%) 3.0507\n",
            "33850 112%) 3.5344\n",
            "33900 112%) 3.3111\n",
            "33950 113%) 3.1606\n",
            "34000 113%) 3.3806\n",
            "34050 113%) 3.5101\n",
            "34100 113%) 3.2762\n",
            "34150 113%) 3.1191\n",
            "34200 113%) 3.6622\n",
            "34250 114%) 3.5343\n",
            "34300 114%) 3.3746\n",
            "34350 114%) 3.3422\n",
            "34400 114%) 3.2526\n",
            "34450 114%) 3.6965\n",
            "34500 114%) 3.3811\n",
            "34550 115%) 3.2278\n",
            "34600 115%) 3.6063\n",
            "34650 115%) 3.1394\n",
            "34700 115%) 3.7971\n",
            "34750 115%) 3.2924\n",
            "34800 115%) 3.6107\n",
            "34850 116%) 3.4665\n",
            "34900 116%) 3.1698\n",
            "34950 116%) 3.3457\n",
            "35000 116%) 3.4261\n",
            "35050 116%) 3.0840\n",
            "35100 117%) 3.4737\n",
            "35150 117%) 3.4665\n",
            "35200 117%) 3.2732\n",
            "35250 117%) 3.4293\n",
            "35300 117%) 3.9109\n",
            "35350 117%) 3.3575\n",
            "35400 118%) 3.5028\n",
            "35450 118%) 3.2240\n",
            "35500 118%) 3.3327\n",
            "35550 118%) 3.4527\n",
            "35600 118%) 3.6699\n",
            "35650 118%) 3.3816\n",
            "35700 119%) 3.2389\n",
            "35750 119%) 3.1577\n",
            "35800 119%) 3.2986\n",
            "35850 119%) 3.7186\n",
            "35900 119%) 3.4652\n",
            "35950 119%) 3.4194\n",
            "36000 120%) 3.1746\n",
            "36050 120%) 3.2225\n",
            "36100 120%) 3.5254\n",
            "36150 120%) 3.6205\n",
            "36200 120%) 3.3817\n",
            "36250 120%) 3.4421\n",
            "36300 121%) 3.2944\n",
            "36350 121%) 2.9980\n",
            "36400 121%) 3.5657\n",
            "36450 121%) 3.1364\n",
            "36500 121%) 3.6998\n",
            "36550 121%) 3.3163\n",
            "36600 122%) 3.1825\n",
            "36650 122%) 3.4224\n",
            "36700 122%) 3.4949\n",
            "36750 122%) 3.1747\n",
            "36800 122%) 3.4880\n",
            "36850 122%) 3.1808\n",
            "36900 123%) 3.6187\n",
            "36950 123%) 3.5909\n",
            "37000 123%) 3.4961\n",
            "37050 123%) 3.6828\n",
            "37100 123%) 3.5116\n",
            "37150 123%) 3.6299\n",
            "37200 124%) 3.3423\n",
            "37250 124%) 3.5050\n",
            "37300 124%) 3.2880\n",
            "37350 124%) 3.3578\n",
            "37400 124%) 3.6395\n",
            "37450 124%) 3.1936\n",
            "37500 125%) 3.3423\n",
            "37550 125%) 3.5499\n",
            "37600 125%) 3.2243\n",
            "37650 125%) 3.3624\n",
            "37700 125%) 3.4673\n",
            "37750 125%) 3.7757\n",
            "37800 126%) 3.3374\n",
            "37850 126%) 3.4513\n",
            "37900 126%) 3.4871\n",
            "37950 126%) 3.2974\n",
            "38000 126%) 3.6039\n",
            "38050 126%) 3.6828\n",
            "38100 127%) 3.6129\n",
            "38150 127%) 3.5948\n",
            "38200 127%) 3.3428\n",
            "38250 127%) 3.3073\n",
            "38300 127%) 3.0526\n",
            "38350 127%) 3.5497\n",
            "38400 128%) 3.1324\n",
            "38450 128%) 3.5829\n",
            "38500 128%) 3.4188\n",
            "38550 128%) 3.4418\n",
            "38600 128%) 3.6063\n",
            "38650 128%) 3.5101\n",
            "38700 129%) 3.3580\n",
            "38750 129%) 3.6593\n",
            "38800 129%) 3.2644\n",
            "38850 129%) 3.5137\n",
            "38900 129%) 3.2124\n",
            "38950 129%) 3.1777\n",
            "39000 130%) 3.3354\n",
            "39050 130%) 3.3957\n",
            "39100 130%) 3.2733\n",
            "39150 130%) 3.1861\n",
            "39200 130%) 3.5029\n",
            "39250 130%) 3.2497\n",
            "39300 131%) 3.0906\n",
            "39350 131%) 3.3628\n",
            "39400 131%) 3.4437\n",
            "39450 131%) 3.3266\n",
            "39500 131%) 3.3723\n",
            "39550 131%) 3.4242\n",
            "39600 132%) 3.5637\n",
            "39650 132%) 3.5124\n",
            "39700 132%) 3.5426\n",
            "39750 132%) 3.3589\n",
            "39800 132%) 3.3157\n",
            "39850 132%) 3.3752\n",
            "39900 133%) 3.1195\n",
            "39950 133%) 3.4371\n",
            "40000 133%) 3.3477\n",
            "40050 133%) 3.4973\n",
            "40100 133%) 3.1287\n",
            "40150 133%) 3.1570\n",
            "40200 134%) 3.4065\n",
            "40250 134%) 3.3339\n",
            "40300 134%) 3.5105\n",
            "40350 134%) 3.1539\n",
            "40400 134%) 3.5722\n",
            "40450 134%) 3.3477\n",
            "40500 135%) 3.2861\n",
            "40550 135%) 3.2897\n",
            "40600 135%) 3.5111\n",
            "40650 135%) 3.6088\n",
            "40700 135%) 3.5076\n",
            "40750 135%) 3.5614\n",
            "40800 136%) 3.3294\n",
            "40850 136%) 3.3085\n",
            "40900 136%) 3.5845\n",
            "40950 136%) 3.4740\n",
            "41000 136%) 3.0563\n",
            "41050 136%) 3.4022\n",
            "41100 137%) 3.4233\n",
            "41150 137%) 3.1734\n",
            "41200 137%) 3.5640\n",
            "41250 137%) 3.5699\n",
            "41300 137%) 3.2093\n",
            "41350 137%) 3.1088\n",
            "41400 138%) 3.4078\n",
            "41450 138%) 3.5586\n",
            "41500 138%) 3.2143\n",
            "41550 138%) 3.2508\n",
            "41600 138%) 3.3590\n",
            "41650 138%) 3.3100\n",
            "41700 139%) 3.3841\n",
            "41750 139%) 3.1979\n",
            "41800 139%) 3.2521\n",
            "41850 139%) 3.4807\n",
            "41900 139%) 3.2760\n",
            "41950 139%) 3.3241\n",
            "42000 140%) 3.3547\n",
            "42050 140%) 3.3383\n",
            "42100 140%) 3.2585\n",
            "42150 140%) 3.2887\n",
            "42200 140%) 3.5974\n",
            "42250 140%) 3.0222\n",
            "42300 141%) 3.2262\n",
            "42350 141%) 3.1571\n",
            "42400 141%) 3.4448\n",
            "42450 141%) 3.1246\n",
            "42500 141%) 3.3616\n",
            "42550 141%) 3.4235\n",
            "42600 142%) 3.4778\n",
            "42650 142%) 3.5441\n",
            "42700 142%) 3.2224\n",
            "42750 142%) 3.6008\n",
            "42800 142%) 3.3799\n",
            "42850 142%) 3.2464\n",
            "42900 143%) 3.3265\n",
            "42950 143%) 3.3678\n",
            "43000 143%) 3.5224\n",
            "43050 143%) 3.4477\n",
            "43100 143%) 3.1867\n",
            "43150 143%) 3.2366\n",
            "43200 144%) 3.4458\n",
            "43250 144%) 3.4672\n",
            "43300 144%) 2.9261\n",
            "43350 144%) 3.4737\n",
            "43400 144%) 3.4547\n",
            "43450 144%) 3.6624\n",
            "43500 145%) 3.6183\n",
            "43550 145%) 3.3922\n",
            "43600 145%) 2.9859\n",
            "43650 145%) 3.2254\n",
            "43700 145%) 3.5120\n",
            "43750 145%) 3.0765\n",
            "43800 146%) 3.3460\n",
            "43850 146%) 3.5448\n",
            "43900 146%) 3.3489\n",
            "43950 146%) 2.9897\n",
            "44000 146%) 3.4105\n",
            "44050 146%) 3.5064\n",
            "44100 147%) 3.3318\n",
            "44150 147%) 3.5207\n",
            "44200 147%) 3.2232\n",
            "44250 147%) 3.3904\n",
            "44300 147%) 2.9474\n",
            "44350 147%) 3.2450\n",
            "44400 148%) 3.4039\n",
            "44450 148%) 3.2982\n",
            "44500 148%) 3.2458\n",
            "44550 148%) 3.6709\n",
            "44600 148%) 3.3745\n",
            "44650 148%) 3.4486\n",
            "44700 149%) 3.2318\n",
            "44750 149%) 3.6215\n",
            "44800 149%) 3.3250\n",
            "44850 149%) 3.4247\n",
            "44900 149%) 3.3232\n",
            "44950 149%) 3.3287\n",
            "45000 150%) 3.1173\n",
            "45050 150%) 3.1047\n",
            "45100 150%) 3.5688\n",
            "45150 150%) 3.4212\n",
            "45200 150%) 3.3828\n",
            "45250 150%) 3.3756\n",
            "45300 151%) 3.3612\n",
            "45350 151%) 2.9465\n",
            "45400 151%) 3.5729\n",
            "45450 151%) 3.1877\n",
            "45500 151%) 3.4055\n",
            "45550 151%) 3.0804\n",
            "45600 152%) 3.5595\n",
            "45650 152%) 3.3458\n",
            "45700 152%) 3.3059\n",
            "45750 152%) 3.6937\n",
            "45800 152%) 3.2242\n",
            "45850 152%) 3.3810\n",
            "45900 153%) 3.4692\n",
            "45950 153%) 3.1880\n",
            "46000 153%) 3.3897\n",
            "46050 153%) 3.2846\n",
            "46100 153%) 3.6039\n",
            "46150 153%) 3.4929\n",
            "46200 154%) 3.3301\n",
            "46250 154%) 3.4954\n",
            "46300 154%) 3.3003\n",
            "46350 154%) 3.1721\n",
            "46400 154%) 3.4134\n",
            "46450 154%) 3.3756\n",
            "46500 155%) 3.3691\n",
            "46550 155%) 3.0989\n",
            "46600 155%) 3.1954\n",
            "46650 155%) 3.4375\n",
            "46700 155%) 3.2809\n",
            "46750 155%) 3.4943\n",
            "46800 156%) 3.6943\n",
            "46850 156%) 3.1806\n",
            "46900 156%) 3.3226\n",
            "46950 156%) 3.6349\n",
            "47000 156%) 3.8468\n",
            "47050 156%) 3.6791\n",
            "47100 157%) 3.6307\n",
            "47150 157%) 3.3786\n",
            "47200 157%) 3.5314\n",
            "47250 157%) 3.4686\n",
            "47300 157%) 3.0142\n",
            "47350 157%) 3.6559\n",
            "47400 158%) 3.3315\n",
            "47450 158%) 3.5673\n",
            "47500 158%) 3.1878\n",
            "47550 158%) 3.4790\n",
            "47600 158%) 3.0005\n",
            "47650 158%) 3.4099\n",
            "47700 159%) 3.4306\n",
            "47750 159%) 3.3742\n",
            "47800 159%) 3.5851\n",
            "47850 159%) 3.4901\n",
            "47900 159%) 3.4963\n",
            "47950 159%) 3.1809\n",
            "48000 160%) 3.4007\n",
            "48050 160%) 2.9111\n",
            "48100 160%) 3.2862\n",
            "48150 160%) 3.5220\n",
            "48200 160%) 3.4271\n",
            "48250 160%) 3.5482\n",
            "48300 161%) 3.3042\n",
            "48350 161%) 3.3092\n",
            "48400 161%) 3.3568\n",
            "48450 161%) 3.6052\n",
            "48500 161%) 3.6233\n",
            "48550 161%) 3.0868\n",
            "48600 162%) 3.2729\n",
            "48650 162%) 3.5180\n",
            "48700 162%) 3.4783\n",
            "48750 162%) 3.4040\n",
            "48800 162%) 3.6174\n",
            "48850 162%) 3.3343\n",
            "48900 163%) 3.3448\n",
            "48950 163%) 3.3601\n",
            "49000 163%) 3.2804\n",
            "49050 163%) 3.0705\n",
            "49100 163%) 3.5926\n",
            "49150 163%) 3.4405\n",
            "49200 164%) 3.1315\n",
            "49250 164%) 2.9726\n",
            "49300 164%) 3.4933\n",
            "49350 164%) 3.2367\n",
            "49400 164%) 3.5915\n",
            "49450 164%) 3.1826\n",
            "49500 165%) 3.2896\n",
            "49550 165%) 3.6970\n",
            "49600 165%) 3.2128\n",
            "49650 165%) 3.0622\n",
            "49700 165%) 3.4664\n",
            "49750 165%) 3.5996\n",
            "49800 166%) 3.5064\n",
            "49850 166%) 3.6045\n",
            "49900 166%) 3.2707\n",
            "49950 166%) 3.3774\n",
            "50000 166%) 3.3535\n",
            "50050 166%) 3.5397\n",
            "50100 167%) 3.1515\n",
            "50150 167%) 3.5371\n",
            "50200 167%) 3.2941\n",
            "50250 167%) 3.2098\n",
            "50300 167%) 3.3352\n",
            "50350 167%) 3.5187\n",
            "50400 168%) 3.2477\n",
            "50450 168%) 3.5956\n",
            "50500 168%) 3.1720\n",
            "50550 168%) 3.4216\n",
            "50600 168%) 3.4094\n",
            "50650 168%) 3.4794\n",
            "50700 169%) 3.5669\n",
            "50750 169%) 3.2317\n",
            "50800 169%) 3.6993\n",
            "50850 169%) 3.0448\n",
            "50900 169%) 3.3447\n",
            "50950 169%) 3.3970\n",
            "51000 170%) 3.3138\n",
            "51050 170%) 3.1555\n",
            "51100 170%) 3.1617\n",
            "51150 170%) 3.4711\n",
            "51200 170%) 3.3342\n",
            "51250 170%) 3.5835\n",
            "51300 171%) 3.5223\n",
            "51350 171%) 3.0506\n",
            "51400 171%) 3.4320\n",
            "51450 171%) 3.5227\n",
            "51500 171%) 3.5530\n",
            "51550 171%) 3.2702\n",
            "51600 172%) 3.7293\n",
            "51650 172%) 3.3865\n",
            "51700 172%) 3.5304\n",
            "51750 172%) 3.4638\n",
            "51800 172%) 3.6556\n",
            "51850 172%) 3.6542\n",
            "51900 173%) 3.2280\n",
            "51950 173%) 3.3139\n",
            "52000 173%) 3.2380\n",
            "52050 173%) 3.2017\n",
            "52100 173%) 3.2419\n",
            "52150 173%) 3.3553\n",
            "52200 174%) 3.2862\n",
            "52250 174%) 3.3676\n",
            "52300 174%) 3.0315\n",
            "52350 174%) 3.0637\n",
            "52400 174%) 3.4108\n",
            "52450 174%) 3.1282\n",
            "52500 175%) 3.1176\n",
            "52550 175%) 3.1945\n",
            "52600 175%) 3.3274\n",
            "52650 175%) 3.2728\n",
            "52700 175%) 3.3169\n",
            "52750 175%) 3.2465\n",
            "52800 176%) 3.6844\n",
            "52850 176%) 3.2077\n",
            "52900 176%) 3.2242\n",
            "52950 176%) 3.5855\n",
            "53000 176%) 3.1762\n",
            "53050 176%) 3.7254\n",
            "53100 177%) 3.2667\n",
            "53150 177%) 3.2841\n",
            "53200 177%) 3.4790\n",
            "53250 177%) 3.6364\n",
            "53300 177%) 3.7054\n",
            "53350 177%) 3.3633\n",
            "53400 178%) 3.2187\n",
            "53450 178%) 3.2402\n",
            "53500 178%) 3.2301\n",
            "53550 178%) 3.7199\n",
            "53600 178%) 3.3449\n",
            "53650 178%) 3.2692\n",
            "53700 179%) 3.0502\n",
            "53750 179%) 3.5983\n",
            "53800 179%) 3.5916\n",
            "53850 179%) 3.1811\n",
            "53900 179%) 3.2638\n",
            "53950 179%) 3.4885\n",
            "54000 180%) 3.2381\n",
            "54050 180%) 3.0761\n",
            "54100 180%) 3.3532\n",
            "54150 180%) 3.3332\n",
            "54200 180%) 3.2111\n",
            "54250 180%) 3.1112\n",
            "54300 181%) 3.2839\n",
            "54350 181%) 3.1579\n",
            "54400 181%) 3.1523\n",
            "54450 181%) 3.3165\n",
            "54500 181%) 3.4433\n",
            "54550 181%) 3.3309\n",
            "54600 182%) 3.1252\n",
            "54650 182%) 3.5222\n",
            "54700 182%) 3.0290\n",
            "54750 182%) 3.3522\n",
            "54800 182%) 3.3294\n",
            "54850 182%) 3.4717\n",
            "54900 183%) 3.2426\n",
            "54950 183%) 3.2232\n",
            "55000 183%) 3.4749\n",
            "55050 183%) 3.4090\n",
            "55100 183%) 2.9939\n",
            "55150 183%) 3.3360\n",
            "55200 184%) 3.1619\n",
            "55250 184%) 3.8555\n",
            "55300 184%) 3.1110\n",
            "55350 184%) 3.5568\n",
            "55400 184%) 3.3175\n",
            "55450 184%) 3.4682\n",
            "55500 185%) 3.2810\n",
            "55550 185%) 3.5011\n",
            "55600 185%) 3.6217\n",
            "55650 185%) 3.2528\n",
            "55700 185%) 3.1350\n",
            "55750 185%) 3.5766\n",
            "55800 186%) 3.3803\n",
            "55850 186%) 3.5001\n",
            "55900 186%) 3.3677\n",
            "55950 186%) 3.3444\n",
            "56000 186%) 3.2071\n",
            "56050 186%) 2.8752\n",
            "56100 187%) 3.3119\n",
            "56150 187%) 3.3818\n",
            "56200 187%) 3.3714\n",
            "56250 187%) 3.2458\n",
            "56300 187%) 3.3874\n",
            "56350 187%) 3.2221\n",
            "56400 188%) 3.8177\n",
            "56450 188%) 3.4681\n",
            "56500 188%) 3.2356\n",
            "56550 188%) 3.1208\n",
            "56600 188%) 3.6065\n",
            "56650 188%) 3.3815\n",
            "56700 189%) 3.2863\n",
            "56750 189%) 3.4225\n",
            "56800 189%) 3.2993\n",
            "56850 189%) 2.9949\n",
            "56900 189%) 3.3045\n",
            "56950 189%) 3.2194\n",
            "57000 190%) 3.3966\n",
            "57050 190%) 3.2249\n",
            "57100 190%) 3.5856\n",
            "57150 190%) 3.3965\n",
            "57200 190%) 3.1881\n",
            "57250 190%) 3.0126\n",
            "57300 191%) 3.4384\n",
            "57350 191%) 3.1454\n",
            "57400 191%) 3.3609\n",
            "57450 191%) 3.2972\n",
            "57500 191%) 3.1888\n",
            "57550 191%) 2.9356\n",
            "57600 192%) 3.4435\n",
            "57650 192%) 3.2261\n",
            "57700 192%) 3.3773\n",
            "57750 192%) 3.2089\n",
            "57800 192%) 3.3642\n",
            "57850 192%) 3.4088\n",
            "57900 193%) 3.1761\n",
            "57950 193%) 3.0340\n",
            "58000 193%) 3.4303\n",
            "58050 193%) 3.4383\n",
            "58100 193%) 3.3796\n",
            "58150 193%) 3.2307\n",
            "58200 194%) 3.4478\n",
            "58250 194%) 3.2892\n",
            "58300 194%) 3.0719\n",
            "58350 194%) 3.3628\n",
            "58400 194%) 3.1724\n",
            "58450 194%) 2.9665\n",
            "58500 195%) 3.3744\n",
            "58550 195%) 3.1966\n",
            "58600 195%) 3.4509\n",
            "58650 195%) 3.2826\n",
            "58700 195%) 3.3618\n",
            "58750 195%) 3.4514\n",
            "58800 196%) 3.5006\n",
            "58850 196%) 3.4193\n",
            "58900 196%) 3.4341\n",
            "58950 196%) 3.0998\n",
            "59000 196%) 3.4626\n",
            "59050 196%) 3.4766\n",
            "59100 197%) 2.9292\n",
            "59150 197%) 3.5219\n",
            "59200 197%) 3.0801\n",
            "59250 197%) 3.3792\n",
            "59300 197%) 3.3488\n",
            "59350 197%) 3.4290\n",
            "59400 198%) 3.0695\n",
            "59450 198%) 3.6792\n",
            "59500 198%) 3.1690\n",
            "59550 198%) 3.3766\n",
            "59600 198%) 3.0997\n",
            "59650 198%) 3.2046\n",
            "59700 199%) 3.5640\n",
            "59750 199%) 3.0206\n",
            "59800 199%) 3.2204\n",
            "59850 199%) 3.3181\n",
            "59900 199%) 3.6064\n",
            "59950 199%) 3.2914\n",
            "60000 200%) 3.0784\n",
            "60050 200%) 3.4183\n",
            "60100 200%) 3.2203\n",
            "60150 200%) 3.3828\n",
            "60200 200%) 3.2858\n",
            "60250 200%) 3.6441\n",
            "60300 200%) 3.6039\n",
            "60350 201%) 3.2856\n",
            "60400 201%) 2.8883\n",
            "60450 201%) 3.3463\n",
            "60500 201%) 3.4553\n",
            "60550 201%) 3.0740\n",
            "60600 202%) 3.2251\n",
            "60650 202%) 3.3118\n",
            "60700 202%) 3.2187\n",
            "60750 202%) 3.0869\n",
            "60800 202%) 3.2535\n",
            "60850 202%) 3.5493\n",
            "60900 202%) 3.7378\n",
            "60950 203%) 3.5605\n",
            "61000 203%) 3.1521\n",
            "61050 203%) 3.2695\n",
            "61100 203%) 3.5395\n",
            "61150 203%) 2.9642\n",
            "61200 204%) 3.6698\n",
            "61250 204%) 3.2765\n",
            "61300 204%) 3.4697\n",
            "61350 204%) 3.3918\n",
            "61400 204%) 3.1740\n",
            "61450 204%) 3.4716\n",
            "61500 204%) 3.3163\n",
            "61550 205%) 3.4956\n",
            "61600 205%) 3.2351\n",
            "61650 205%) 3.3372\n",
            "61700 205%) 3.4604\n",
            "61750 205%) 3.4995\n",
            "61800 206%) 3.2991\n",
            "61850 206%) 3.3981\n",
            "61900 206%) 3.2592\n",
            "61950 206%) 3.4883\n",
            "62000 206%) 3.0445\n",
            "62050 206%) 3.1206\n",
            "62100 206%) 3.1853\n",
            "62150 207%) 3.4626\n",
            "62200 207%) 3.6803\n",
            "62250 207%) 3.3025\n",
            "62300 207%) 3.4120\n",
            "62350 207%) 3.0434\n",
            "62400 208%) 3.0639\n",
            "62450 208%) 3.1428\n",
            "62500 208%) 3.2199\n",
            "62550 208%) 3.4514\n",
            "62600 208%) 3.4470\n",
            "62650 208%) 2.9788\n",
            "62700 209%) 3.1327\n",
            "62750 209%) 3.2857\n",
            "62800 209%) 3.5140\n",
            "62850 209%) 3.5256\n",
            "62900 209%) 3.5123\n",
            "62950 209%) 3.3725\n",
            "63000 210%) 3.3056\n",
            "63050 210%) 3.3391\n",
            "63100 210%) 3.2512\n",
            "63150 210%) 3.1853\n",
            "63200 210%) 3.0654\n",
            "63250 210%) 3.2506\n",
            "63300 211%) 3.3186\n",
            "63350 211%) 3.3829\n",
            "63400 211%) 3.4229\n",
            "63450 211%) 3.4465\n",
            "63500 211%) 3.2215\n",
            "63550 211%) 3.3551\n",
            "63600 212%) 3.1148\n",
            "63650 212%) 3.3946\n",
            "63700 212%) 3.3842\n",
            "63750 212%) 3.1260\n",
            "63800 212%) 3.5159\n",
            "63850 212%) 3.5771\n",
            "63900 213%) 3.5738\n",
            "63950 213%) 3.3230\n",
            "64000 213%) 3.4127\n",
            "64050 213%) 3.3683\n",
            "64100 213%) 3.1830\n",
            "64150 213%) 3.1114\n",
            "64200 214%) 3.2115\n",
            "64250 214%) 3.1082\n",
            "64300 214%) 3.5584\n",
            "64350 214%) 3.4900\n",
            "64400 214%) 3.3987\n",
            "64450 214%) 3.2769\n",
            "64500 215%) 3.6046\n",
            "64550 215%) 3.1825\n",
            "64600 215%) 3.6130\n",
            "64650 215%) 3.2764\n",
            "64700 215%) 3.1310\n",
            "64750 215%) 2.9611\n",
            "64800 216%) 3.4937\n",
            "64850 216%) 3.3970\n",
            "64900 216%) 3.3360\n",
            "64950 216%) 3.2150\n",
            "65000 216%) 3.1478\n",
            "65050 216%) 3.5521\n",
            "65100 217%) 3.4814\n",
            "65150 217%) 3.2657\n",
            "65200 217%) 3.3584\n",
            "65250 217%) 3.1168\n",
            "65300 217%) 3.7130\n",
            "65350 217%) 3.2469\n",
            "65400 218%) 3.4403\n",
            "65450 218%) 3.2230\n",
            "65500 218%) 3.2583\n",
            "65550 218%) 3.3599\n",
            "65600 218%) 3.3551\n",
            "65650 218%) 3.2204\n",
            "65700 219%) 3.3017\n",
            "65750 219%) 3.2765\n",
            "65800 219%) 3.5253\n",
            "65850 219%) 3.4387\n",
            "65900 219%) 3.4129\n",
            "65950 219%) 3.3959\n",
            "66000 220%) 3.3736\n",
            "66050 220%) 3.2819\n",
            "66100 220%) 3.2205\n",
            "66150 220%) 3.5737\n",
            "66200 220%) 3.1944\n",
            "66250 220%) 3.4578\n",
            "66300 221%) 3.6322\n",
            "66350 221%) 3.1408\n",
            "66400 221%) 3.4184\n",
            "66450 221%) 3.1308\n",
            "66500 221%) 3.3020\n",
            "66550 221%) 3.3911\n",
            "66600 222%) 3.2458\n",
            "66650 222%) 3.1179\n",
            "66700 222%) 3.2154\n",
            "66750 222%) 3.4653\n",
            "66800 222%) 3.4130\n",
            "66850 222%) 3.2101\n",
            "66900 223%) 3.2198\n",
            "66950 223%) 3.5153\n",
            "67000 223%) 3.5175\n",
            "67050 223%) 3.2942\n",
            "67100 223%) 3.5493\n",
            "67150 223%) 3.3709\n",
            "67200 224%) 3.1931\n",
            "67250 224%) 3.5187\n",
            "67300 224%) 3.5209\n",
            "67350 224%) 3.2141\n",
            "67400 224%) 3.4267\n",
            "67450 224%) 3.3710\n",
            "67500 225%) 3.5356\n",
            "67550 225%) 3.2040\n",
            "67600 225%) 3.3242\n",
            "67650 225%) 3.2966\n",
            "67700 225%) 3.0405\n",
            "67750 225%) 3.3668\n",
            "67800 225%) 3.2658\n",
            "67850 226%) 3.3686\n",
            "67900 226%) 3.1563\n",
            "67950 226%) 3.3900\n",
            "68000 226%) 3.6112\n",
            "68050 226%) 3.0619\n",
            "68100 227%) 3.2213\n",
            "68150 227%) 3.2146\n",
            "68200 227%) 3.1449\n",
            "68250 227%) 3.1388\n",
            "68300 227%) 3.5092\n",
            "68350 227%) 3.1212\n",
            "68400 227%) 3.1926\n",
            "68450 228%) 3.4463\n",
            "68500 228%) 3.5981\n",
            "68550 228%) 3.3903\n",
            "68600 228%) 3.4350\n",
            "68650 228%) 3.2342\n",
            "68700 229%) 3.4462\n",
            "68750 229%) 3.3497\n",
            "68800 229%) 3.4833\n",
            "68850 229%) 3.1640\n",
            "68900 229%) 3.1737\n",
            "68950 229%) 3.1004\n",
            "69000 229%) 3.3765\n",
            "69050 230%) 3.5078\n",
            "69100 230%) 3.3549\n",
            "69150 230%) 3.1444\n",
            "69200 230%) 3.1556\n",
            "69250 230%) 3.2707\n",
            "69300 231%) 3.0090\n",
            "69350 231%) 3.3729\n",
            "69400 231%) 3.1208\n",
            "69450 231%) 3.1501\n",
            "69500 231%) 3.3267\n",
            "69550 231%) 3.2171\n",
            "69600 231%) 3.3910\n",
            "69650 232%) 3.3356\n",
            "69700 232%) 3.3010\n",
            "69750 232%) 3.6337\n",
            "69800 232%) 3.3450\n",
            "69850 232%) 3.2417\n",
            "69900 233%) 3.2394\n",
            "69950 233%) 3.7436\n",
            "70000 233%) 3.1764\n",
            "70050 233%) 3.3068\n",
            "70100 233%) 3.5320\n",
            "70150 233%) 3.1035\n",
            "70200 234%) 3.2045\n",
            "70250 234%) 3.2654\n",
            "70300 234%) 3.0338\n",
            "70350 234%) 3.6319\n",
            "70400 234%) 3.1747\n",
            "70450 234%) 3.3149\n",
            "70500 235%) 3.2669\n",
            "70550 235%) 3.2126\n",
            "70600 235%) 3.1494\n",
            "70650 235%) 3.2408\n",
            "70700 235%) 3.2696\n",
            "70750 235%) 3.1704\n",
            "70800 236%) 3.5720\n",
            "70850 236%) 3.2573\n",
            "70900 236%) 3.4436\n",
            "70950 236%) 3.1948\n",
            "71000 236%) 3.2456\n",
            "71050 236%) 3.4864\n",
            "71100 237%) 3.1444\n",
            "71150 237%) 3.3647\n",
            "71200 237%) 3.6378\n",
            "71250 237%) 3.2595\n",
            "71300 237%) 3.4568\n",
            "71350 237%) 3.1137\n",
            "71400 238%) 3.5592\n",
            "71450 238%) 3.5597\n",
            "71500 238%) 3.1696\n",
            "71550 238%) 3.4533\n",
            "71600 238%) 3.2437\n",
            "71650 238%) 3.4049\n",
            "71700 239%) 3.1820\n",
            "71750 239%) 3.2460\n",
            "71800 239%) 3.1720\n",
            "71850 239%) 3.4985\n",
            "71900 239%) 3.4873\n",
            "71950 239%) 2.9797\n",
            "72000 240%) 3.3920\n",
            "72050 240%) 3.3772\n",
            "72100 240%) 3.5737\n",
            "72150 240%) 3.5864\n",
            "72200 240%) 3.2594\n",
            "72250 240%) 3.1637\n",
            "72300 241%) 3.2817\n",
            "72350 241%) 3.1289\n",
            "72400 241%) 3.4149\n",
            "72450 241%) 3.1934\n",
            "72500 241%) 3.5647\n",
            "72550 241%) 3.6628\n",
            "72600 242%) 3.4240\n",
            "72650 242%) 3.4125\n",
            "72700 242%) 3.6670\n",
            "72750 242%) 3.2533\n",
            "72800 242%) 3.6870\n",
            "72850 242%) 3.5291\n",
            "72900 243%) 3.1963\n",
            "72950 243%) 3.2049\n",
            "73000 243%) 3.4902\n",
            "73050 243%) 3.2472\n",
            "73100 243%) 3.4659\n",
            "73150 243%) 3.3880\n",
            "73200 244%) 3.2829\n",
            "73250 244%) 3.1206\n",
            "73300 244%) 3.2255\n",
            "73350 244%) 3.1611\n",
            "73400 244%) 3.1915\n",
            "73450 244%) 3.5516\n",
            "73500 245%) 3.3780\n",
            "73550 245%) 3.3118\n",
            "73600 245%) 3.3568\n",
            "73650 245%) 3.2133\n",
            "73700 245%) 3.3549\n",
            "73750 245%) 3.3162\n",
            "73800 246%) 3.5060\n",
            "73850 246%) 2.9562\n",
            "73900 246%) 3.3277\n",
            "73950 246%) 3.4046\n",
            "74000 246%) 3.4729\n",
            "74050 246%) 3.0902\n",
            "74100 247%) 3.2261\n",
            "74150 247%) 3.1276\n",
            "74200 247%) 3.5098\n",
            "74250 247%) 3.1528\n",
            "74300 247%) 3.2870\n",
            "74350 247%) 3.6748\n",
            "74400 248%) 3.1002\n",
            "74450 248%) 3.2889\n",
            "74500 248%) 3.4265\n",
            "74550 248%) 3.4260\n",
            "74600 248%) 3.4107\n",
            "74650 248%) 3.2968\n",
            "74700 249%) 2.9847\n",
            "74750 249%) 3.2397\n",
            "74800 249%) 3.5427\n",
            "74850 249%) 3.3032\n",
            "74900 249%) 3.1156\n",
            "74950 249%) 3.5336\n",
            "75000 250%) 3.1971\n",
            "75050 250%) 3.0326\n",
            "75100 250%) 3.1124\n",
            "75150 250%) 3.4094\n",
            "75200 250%) 3.2118\n",
            "75250 250%) 3.2864\n",
            "75300 250%) 3.6266\n",
            "75350 251%) 3.4933\n",
            "75400 251%) 3.3888\n",
            "75450 251%) 3.2353\n",
            "75500 251%) 3.4159\n",
            "75550 251%) 3.2866\n",
            "75600 252%) 3.2412\n",
            "75650 252%) 3.4856\n",
            "75700 252%) 3.5022\n",
            "75750 252%) 3.3806\n",
            "75800 252%) 3.0946\n",
            "75850 252%) 3.3290\n",
            "75900 252%) 3.3834\n",
            "75950 253%) 3.4353\n",
            "76000 253%) 3.1296\n",
            "76050 253%) 3.6784\n",
            "76100 253%) 3.2038\n",
            "76150 253%) 3.4119\n",
            "76200 254%) 3.5200\n",
            "76250 254%) 3.2179\n",
            "76300 254%) 3.2875\n",
            "76350 254%) 3.1083\n",
            "76400 254%) 3.1158\n",
            "76450 254%) 3.2533\n",
            "76500 254%) 3.2936\n",
            "76550 255%) 3.2509\n",
            "76600 255%) 3.3103\n",
            "76650 255%) 3.5113\n",
            "76700 255%) 3.0953\n",
            "76750 255%) 3.5606\n",
            "76800 256%) 3.7448\n",
            "76850 256%) 3.4160\n",
            "76900 256%) 3.0424\n",
            "76950 256%) 3.3737\n",
            "77000 256%) 3.3116\n",
            "77050 256%) 3.0799\n",
            "77100 257%) 3.4920\n",
            "77150 257%) 3.5945\n",
            "77200 257%) 3.2924\n",
            "77250 257%) 3.2377\n",
            "77300 257%) 3.2936\n",
            "77350 257%) 3.2803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-2629acc1b0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrnn_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2Index_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_source_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_Iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-843f81959e8a>\u001b[0m in \u001b[0;36mtrain_Iters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_source_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c08163655067>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion, max_length, iters)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_over_extended_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mreverse_extended_vocab\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mind2Word_dec_big\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIMBqS1DzSyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6ce5b582-390a-482c-e1f8-2814dace2a19"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iters = list(loss_graph.keys())\n",
        "loss_val = list(loss_graph.values())\n",
        "plt.plot(iters, loss_val)\n",
        "plt.ylim(0,8) \n",
        "plt.xlim(0,77350)\n",
        "plt.xlabel('Iterations') \n",
        "plt.ylabel('Loss')  \n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA9RnrZgk6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        pair1 = torch.tensor(pair[0],dtype=torch.long,device=device)\n",
        "        pair2 = pair[1]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair1)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        output_list = [ind2Word_dec_big[word] for word in pair2]\n",
        "        output_list = ' '.join(output_list)\n",
        "        input_sentence = [ind2Word_enc[element.item()] for element in pair1.flatten()]\n",
        "        input_sentence = ' '.join(input_sentence)\n",
        "        print(\"Sentence is  \",input_sentence)\n",
        "        print('<',output_sentence)\n",
        "        print('=',output_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObARxnAyoUTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, encoder_tensor, max_length=max_source_length):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = encoder_tensor\n",
        "        input_length = input_tensor.size(0)\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        prev_unk_word = ''\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0),\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        extended_vocab = psuInd2Word_dec.copy()\n",
        "        duplicate_words = {}\n",
        "        extend_key = len(word2Index_dec.keys())\n",
        "        input_list = input_tensor.tolist()\n",
        "        i =0\n",
        "        for input_word in input_list:\n",
        "          if ind2Word_enc[input_word] in word2Index_dec.keys():\n",
        "            duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word]]\n",
        "          else:\n",
        "            extended_vocab[extend_key] = ind2Word_enc[input_word]\n",
        "            extend_key += 1\n",
        "          i = i+1\n",
        "\n",
        "        decoder_input = torch.tensor([word2Index_dec['<START>']], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention,pgen = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, prev_unk_word)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
        "\n",
        "            decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
        "            p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
        "            p_duplicate_list = p_duplicate_list.tolist()\n",
        "            for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
        "              p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
        "            p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
        "            p_diag = torch.mm(decoder_attention, p_duplicate)\n",
        "            p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
        "            p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
        "            P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
        "\n",
        "            for i in range(input_length):\n",
        "              if not (1 in p_duplicate_list[i]):\n",
        "                P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
        "\n",
        "            idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
        "            if idx.item() < len(word2Index_dec.keys()):   \n",
        "              decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
        "              decoded_words.append(extended_vocab[idx.item()])\n",
        "            elif idx.item() >= len(word2Index_dec.keys()):\n",
        "              decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
        "              prev_unk_word = extended_vocab[idx.item()]\n",
        "              decoded_words.append(extended_vocab[idx.item()])\n",
        "            if idx.item() == word2Index_dec['<END>']:\n",
        "              decoded_words.append('<END>')\n",
        "              break\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jOiIQfeoJHe",
        "colab_type": "code",
        "outputId": "8493722d-9844-4745-a8ed-1f47b3716f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluateRandomly(rnn_encoder, rnn_decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9997], device='cuda:0')\n",
            "tensor([0.9987], device='cuda:0')\n",
            "tensor([0.9959], device='cuda:0')\n",
            "tensor([0.9942], device='cuda:0')\n",
            "tensor([0.9919], device='cuda:0')\n",
            "Sentence is   tastes as if they colored the peppermint altoids green stronger than original and definitely not spearmint flavored original had pleasant mildly sweet taste have not we learned from coca cola to leave well enough alone\n",
            "< <START> not not not <END> <END>\n",
            "= <START> nasty <END>\n",
            "tensor([0.9985], device='cuda:0')\n",
            "tensor([0.9947], device='cuda:0')\n",
            "tensor([0.9990], device='cuda:0')\n",
            "tensor([0.9995], device='cuda:0')\n",
            "tensor([0.9994], device='cuda:0')\n",
            "tensor([0.9994], device='cuda:0')\n",
            "Sentence is   this chicken was a gift and it arrived on time and was well liked fast service and good quality\n",
            "< <START> very to to to <END> <END>\n",
            "= <START> great quality <END>\n",
            "tensor([0.9986], device='cuda:0')\n",
            "tensor([0.9918], device='cuda:0')\n",
            "tensor([0.9791], device='cuda:0')\n",
            "tensor([0.9670], device='cuda:0')\n",
            "tensor([0.9792], device='cuda:0')\n",
            "tensor([0.9845], device='cuda:0')\n",
            "tensor([0.9757], device='cuda:0')\n",
            "tensor([0.9843], device='cuda:0')\n",
            "Sentence is   these bags had a lot of overcooked brown pieces also felt very greasy had to keep wiping my fingers on a napkin\n",
            "< <START> not of like not get it <END> <END>\n",
            "= <START> burnt <END>\n",
            "tensor([0.9996], device='cuda:0')\n",
            "tensor([0.9981], device='cuda:0')\n",
            "tensor([0.9954], device='cuda:0')\n",
            "tensor([0.9954], device='cuda:0')\n",
            "tensor([0.9967], device='cuda:0')\n",
            "Sentence is   i am torn between giving this 1 star or 5 stars because it is just too good it blows nutella out of the water it lasted less than 24 hours in my household and i live alone it is just too good\n",
            "< <START> good and good <END> <END>\n",
            "= <START> too good <END>\n",
            "tensor([0.9997], device='cuda:0')\n",
            "tensor([0.9986], device='cuda:0')\n",
            "tensor([0.9961], device='cuda:0')\n",
            "tensor([0.9959], device='cuda:0')\n",
            "tensor([0.9970], device='cuda:0')\n",
            "Sentence is   i had purchased the black licorice and thought it to be some of the best i would ever had and i do love licorice ordered this only because the black was not available but found it tastes more like grape of which i am not fond\n",
            "< <START> not not not <END> <END>\n",
            "= <START> grape <END>\n",
            "tensor([0.9992], device='cuda:0')\n",
            "tensor([0.9968], device='cuda:0')\n",
            "tensor([0.9898], device='cuda:0')\n",
            "tensor([0.9928], device='cuda:0')\n",
            "tensor([0.9906], device='cuda:0')\n",
            "Sentence is   did not care much for it it left a gritty coat in my throat and the taste left an aftertaste that was not very pleasant cold or hot either way i do not care for it\n",
            "< <START> not not not <END> <END>\n",
            "= <START> bad aftertaste <END>\n",
            "tensor([0.9996], device='cuda:0')\n",
            "tensor([0.9990], device='cuda:0')\n",
            "tensor([0.9968], device='cuda:0')\n",
            "tensor([0.9943], device='cuda:0')\n",
            "tensor([0.9932], device='cuda:0')\n",
            "tensor([0.9929], device='cuda:0')\n",
            "tensor([0.9915], device='cuda:0')\n",
            "tensor([0.9875], device='cuda:0')\n",
            "Sentence is   this is the 3rd case of jello instant sugar free pudding i ve purcased in the last month and it continues to be the best quality and price of any supermarket that i have shopped i will continue to purchase this item from amazon com jaypaul\n",
            "< <START> great and price price price best <END> <END>\n",
            "= <START> pudding <END>\n",
            "tensor([0.9998], device='cuda:0')\n",
            "tensor([0.9986], device='cuda:0')\n",
            "tensor([0.9955], device='cuda:0')\n",
            "tensor([0.9976], device='cuda:0')\n",
            "Sentence is   i ordered this because one of the reviews said it tasted better than xango definitely not it is very very sweet i even cut it with unsweetened cranberry juice and still sweet good price though\n",
            "< <START> not and <END> <END>\n",
            "= <START> mangosteen <END>\n",
            "tensor([0.9975], device='cuda:0')\n",
            "tensor([0.9904], device='cuda:0')\n",
            "tensor([0.9603], device='cuda:0')\n",
            "tensor([0.9633], device='cuda:0')\n",
            "tensor([0.9489], device='cuda:0')\n",
            "Sentence is   mr altoids please let me send you some spearmint i ve got tons of the stuff in my garden and you are clearly running low whatever it is you have decided to put in the new spearmint altoids it is certainly not much good for eating\n",
            "< <START> not and not <END> <END>\n",
            "= <START> so sad <END>\n",
            "tensor([0.9998], device='cuda:0')\n",
            "tensor([0.9991], device='cuda:0')\n",
            "tensor([0.9969], device='cuda:0')\n",
            "tensor([0.9983], device='cuda:0')\n",
            "Sentence is   this product is excellent and fairly priced i am very happy with the taste and quality it is very gourmet br this kind of salt is good for you of course moderation is the key\n",
            "< <START> good and <END> <END>\n",
            "= <START> wonderful salt <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gKcQZ5Tluv-",
        "colab_type": "code",
        "outputId": "bb7096ca-59bb-429f-b8bf-03441bb2a5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.save(rnn_encoder,'/content/drive/My Drive/News_Summary/nm_a_encoder_attn_ptr_many_iter_30kdata_amazon.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BnqyLNV1b5k",
        "colab_type": "code",
        "outputId": "e6af0cb0-eb7c-4234-86ba-f1800b2d1d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.save(rnn_decoder,'/content/drive/My Drive/News_Summary/nm_a_decoder_attn_ptr_many_iter_30kdata_amazon.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AttentionDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43IIx0YXY8jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_encoder = torch.load('/content/drive/My Drive/News_Summary/nm_a_encoder_attn_ptr_many_iter_30kdata_amazon.h5', map_location=torch.device('cpu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJpHjF-iaBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_decoder = torch.load('/content/drive/My Drive/News_Summary/nm_a_decoder_attn_ptr_many_iter_30kdata_amazon.h5', map_location=torch.device('cpu'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}